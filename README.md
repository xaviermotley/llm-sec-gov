ü§ñ AI/LLM Security & Governance Starter Kit  
Threat‚Äëmodel, test, and govern LLM apps  

Red‚ÄëTeam/Evals Harness  
Simple & NeMo Guardrails  
Governance Pack (NIST AI RMF)  
Metrics & CI  
License: MIT  

# AI/LLM Security & Governance Starter Kit

A complete starter kit to **threat‚Äëmodel, test, and govern LLM apps**. It ships with:  
- **Red‚Äëteam/evals harness** (prompt injection, data exfiltration, unsafe content)  
- **Guardrails** (simple policy filters + optional NVIDIA **NeMo Guardrails**)  
- **Governance artifacts** (NIST **AI RMF** mapping, risk register, PIA, model/system cards)  
- **Metrics** and CI that fail the build on regressions  

> Framework anchors: **NIST AI RMF 1.0** (‚ÄúGovern, Map, Measure, Manage‚Äù), **OWASP Top 10 for LLM Applications (2025)**, **MITRE ATLAS** adversarial techniques, with EU **AI Act** timelines in mind for governance/readiness.

---

## Why this matters now

- **NIST AI RMF 1.0** is the de‚Äëfacto risk framework; we map deliverables to **Govern, Map, Measure, Manage** so auditors and leadership see a familiar structure.  
- **OWASP LLM Top 10 (2025)** provides concrete risks/mitigations (e.g., Prompt Injection, Data Leakage) that our tests exercise.  
- **MITRE ATLAS** catalogs adversarial tactics against AI systems; we use it to label tests and coverage.  
- EU **AI Act** obligations are phasing in through 2026‚Äì2027, with 2025 guidance and GPAI expectations‚Äîthis repo‚Äôs governance pack helps you show active readiness.

(Authoritative sources listed in **References** below.)

---

## What‚Äôs inside

- **Threat models** (`/threat-models`): filled template + example for a ‚Äústorefront support bot‚Äù.  
- **Evals** (`/evals`):  
  - `runner.py` runs tests against any HTTP LLM endpoint (`POST { "prompt": "..." } ‚Üí { "response": "..." }`).  
  - Baseline tests: `/evals/prompts/*.txt` and config in `/evals/configs/baseline.yaml`.  
  - Results saved to `/evals/reports/`.  
- **Guardrails** (`/guardrails`):  
  - Lightweight `simple_filters.py` (regex + policy checks) wired into the evals harness.  
  - Optional **NeMo Guardrails** (`/guardrails/nemo`) with a minimal Colang policy and config.  
- **Governance** (`/governance`):  
  - `nist-airmf/controls-mapping.md` (how repo artifacts map to AI RMF).  
  - `risk-register.csv`, `pia-template.md`, `model-card.md`, `system-card.md`, `release-checklist.md`.  
- **Docs** (`/docs`): architecture, metrics to track, and how‚Äëto‚Äërun.  
- **CI** (`.github/workflows`): Python unit tests + (optional) **CodeQL** and **OpenSSF Scorecard**.  

---

## Quickstart

### 0) Prereqs
- Python **3.11+**  
- Your LLM endpoint (local or hosted) that accepts `{"prompt": "..."} ‚Üí {"response": "..."}`.  
  *Tips:* For local dev, you can mock an endpoint or run any OpenAI‚Äëcompatible API locally.  

### 1) Install  

```bash
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -e .
```

### 2) Run the baseline evals  

```bash
export MODEL_ENDPOINT_URL=http://localhost:8000/v1/chat/completions  # or your endpoint
python evals/runner.py --config evals/configs/baseline.yaml --endpoint $MODEL_ENDPOINT_URL
```

This runs:  
	‚Ä¢	Prompt Injection (attempts to override system policy)  
	‚Ä¢	Data Exfiltration (tries to elicit known secret text)  
	‚Ä¢	Unsafe Content (toxicity/illicit assistance prompts)  

Outputs: machine‚Äëreadable JSON + console summary with pass/fail and guardrail catch‚Äërates.  

### 3) (Optional) Add NeMo Guardrails  

```bash
pip install nemoguardrails
python -m guardrails.nemo_demo   # see guardrails/nemo/ for config and colang
```

### 4) Metrics to publish (copy to your README badges)  
	‚Ä¢	Blocked jailbreaks (% of prompt‚Äëinjection tests blocked)  
	‚Ä¢	Data‚Äëleak prevention (% of seeded secrets never echoed)  
	‚Ä¢	Unsafe content adherence (% blocked or transformed)  
	‚Ä¢	Regression rate (week‚Äëover‚Äëweek failures)  

See /docs/metrics.md for guidance and examples.  

---

## Architecture


---

## Map to NIST AI RMF 1.0
	‚Ä¢	Govern: RACI, release-checklist.md, risk acceptance criteria, system-card.md.  
	‚Ä¢	Map: Threat model templates, data flow diagrams, context of use.  
	‚Ä¢	Measure: Evals harness, coverage vs. OWASP Top 10 & MITRE ATLAS, KPIs in /docs/metrics.md.  
	‚Ä¢	Manage: CI gates, remediation workflow (open issues on failures), documented exceptions & review cadence.  

---

## OWASP & ATLAS Coverage

| Risk | Where tested | Mitigation examples |
| --- | --- | --- |
| **LLM01: Prompt Injection** | prompt_injection.txt | System prompts, output filtering, tool‚Äëcall whitelists |
| **LLM02: Data Leakage** | data_exfiltration.txt (seeded secrets) | Secrets scanning & redaction, retrieval guardrails |
| **LLM0X: Unsafe Outputs** | unsafe_content.txt | Safety classifiers/filters, refusal & safe‚Äëcomplete |

(Adjust per 2025 OWASP LLM Top 10 list; link in References.)

---

## References & learn‚Äëby‚Äëdoing videos

### Primary frameworks & guidance
- **NIST AI RMF 1.0** ‚Äì official PDF + overview.  
- **OWASP Top 10 for LLMs / GenAI Security Project (2025)** ‚Äì risks & mitigations.  
- **MITRE ATLAS** ‚Äì adversarial tactics for AI.  
- **EU AI Act timeline** ‚Äì implementation dates and GPAI guidance context.  

### Open‚Äësource tools you can plug in
- **PyRIT** ‚Äì AI red‚Äëteam automation framework.  
- **garak** ‚Äì LLM vulnerability scanner.  
- **NVIDIA NeMo Guardrails** ‚Äì programmable guardrails for LLM apps.  
- **Llama Prompt Guard 2** ‚Äì lightweight moderation/safety classifier.  

### YouTube: follow‚Äëalong builds
- **AI Red Teaming 101** (full playlist + flagship episode) ‚Äì prompt injection, multi‚Äëturn attacks, automation with PyRIT.  
- **PyRIT deep dives** ‚Äì hands‚Äëon talks/how‚Äëtos.  
- **garak tutorials** ‚Äì run scans against your own API.  
- **NeMo Guardrails walkthroughs** ‚Äì end‚Äëto‚Äëend guardrails.  
- **NIST AI RMF explainers** ‚Äì concise intros.  
- **OWASP LLM Top 10 (2025) videos** ‚Äì updates & short demos.  

---

## Security & CI
	‚Ä¢	Unit tests for filters/policies (/evals/tests).  
	‚Ä¢	CI gates (GitHub Actions) to fail on eval regression.  
	‚Ä¢	Optional: CodeQL and OpenSSF Scorecard workflows.  

---

## Contributing

PRs welcome! Please add new tests under `/evals/prompts`, update `/evals/configs/*.yaml`, and map them to OWASP/ATLAS labels.  

---

## License

MIT (see LICENSE).
