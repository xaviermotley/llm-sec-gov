ðŸ¤– AI/LLM Security & Governance Starter Kit  
Threatâ€‘model, test, and govern LLM apps  

Redâ€‘Team/Evals Harness  
Simple & NeMo Guardrails  
Governance Pack (NIST AI RMF)  
Metrics & CI  
License: MIT  

# AI/LLM Security & Governance Starter Kit

A complete starter kit to **threatâ€‘model, test, and govern LLM apps**. It ships with:  
- **Redâ€‘team/evals harness** (prompt injection, data exfiltration, unsafe content)  
- **Guardrails** (simple policy filters + optional NVIDIA **NeMo Guardrails**)  
- **Governance artifacts** (NIST **AI RMF** mapping, risk register, PIA, model/system cards)  
- **Metrics** and CI that fail the build on regressions  

> Framework anchors: **NIST AI RMF 1.0** (â€œGovern, Map, Measure, Manageâ€), **OWASP Top 10 for LLM Applications (2025)**, **MITRE ATLAS** adversarial techniques, with EU **AI Act** timelines in mind for governance/readiness.

---

## Why this matters now

- **NIST AI RMF 1.0** is the deâ€‘facto risk framework; we map deliverables to **Govern, Map, Measure, Manage** so auditors and leadership see a familiar structure.  
- **OWASP LLM Top 10 (2025)** provides concrete risks/mitigations (e.g., Prompt Injection, Data Leakage) that our tests exercise.  
- **MITRE ATLAS** catalogs adversarial tactics against AI systems; we use it to label tests and coverage.  
- EU **AI Act** obligations are phasing in through 2026â€“2027, with 2025 guidance and GPAI expectationsâ€”this repoâ€™s governance pack helps you show active readiness.

(Authoritative sources listed in **References** below.)

---

## Whatâ€™s inside

- **Threat models** (`/threat-models`): filled template + example for a â€œstorefront support botâ€.  
- **Evals** (`/evals`):  
  - `runner.py` runs tests against any HTTP LLM endpoint (`POST { "prompt": "..." } â†’ { "response": "..." }`).  
  - Baseline tests: `/evals/prompts/*.txt` and config in `/evals/configs/baseline.yaml`.  
  - Results saved to `/evals/reports/`.  
- **Guardrails** (`/guardrails`):  
  - Lightweight `simple_filters.py` (regex + policy checks) wired into the evals harness.  
  - Optional **NeMo Guardrails** (`/guardrails/nemo`) with a minimal Colang policy and config.  
- **Governance** (`/governance`):  
  - `nist-airmf/controls-mapping.md` (how repo artifacts map to AI RMF).  
  - `risk-register.csv`, `pia-template.md`, `model-card.md`, `system-card.md`, `release-checklist.md`.  
- **Docs** (`/docs`): architecture, metrics to track, and howâ€‘toâ€‘run.  
- **CI** (`.github/workflows`): Python unit tests + (optional) **CodeQL** and **OpenSSF Scorecard**.  

---

## Quickstart

### 0) Prereqs
- Python **3.11+**  
- Your LLM endpoint (local or hosted) that accepts `{"prompt": "..."} â†’ {"response": "..."}`.  
  *Tips:* For local dev, you can mock an endpoint or run any OpenAIâ€‘compatible API locally.  

### 1) Install  

```bash
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -e .
```

### 2) Run the baseline evals  

```bash
export MODEL_ENDPOINT_URL=http://localhost:8000/v1/chat/completions  # or your endpoint
python evals/runner.py --config evals/configs/baseline.yaml --endpoint $MODEL_ENDPOINT_URL
```

This runs:  
	â€¢	Prompt Injection (attempts to override system policy)  
	â€¢	Data Exfiltration (tries to elicit known secret text)  
	â€¢	Unsafe Content (toxicity/illicit assistance prompts)  

Outputs: machineâ€‘readable JSON + console summary with pass/fail and guardrail catchâ€‘rates.  

### 3) (Optional) Add NeMo Guardrails  

```bash
pip install nemoguardrails
python -m guardrails.nemo_demo   # see guardrails/nemo/ for config and colang
```

### 4) Metrics to publish (copy to your README badges)  
	â€¢	Blocked jailbreaks (% of promptâ€‘injection tests blocked)  
	â€¢	Dataâ€‘leak prevention (% of seeded secrets never echoed)  
	â€¢	Unsafe content adherence (% blocked or transformed)  
	â€¢	Regression rate (weekâ€‘overâ€‘week failures)  

See /docs/metrics.md for guidance and examples.  

---

## Architecture

```mermaid
flowchart LR
  A[Attack prompts\n(evals/prompts)] --> B[runner.py\n(test harness)]
  B --> C[LLM endpoint\n(local/remote)]
  B --> D[Guardrails\n(simple_filters / NeMo)]
  C -->|response| D -->|checked response| E[Reports\n(evals/reports)]
  B --> F[Governance artifacts\n(risk register, AI RMF map)]
  B --> G[CI gates\n(fail on regression)]
```

---

## Map to NIST AI RMF 1.0
	â€¢	Govern: RACI, release-checklist.md, risk acceptance criteria, system-card.md.  
	â€¢	Map: Threat model templates, data flow diagrams, context of use.  
	â€¢	Measure: Evals harness, coverage vs. OWASP Top 10 & MITRE ATLAS, KPIs in /docs/metrics.md.  
	â€¢	Manage: CI gates, remediation workflow (open issues on failures), documented exceptions & review cadence.  

---

## OWASP & ATLAS Coverage

| Risk | Where tested | Mitigation examples |
| --- | --- | --- |
| **LLM01: Prompt Injection** | prompt_injection.txt | System prompts, output filtering, toolâ€‘call whitelists |
| **LLM02: Data Leakage** | data_exfiltration.txt (seeded secrets) | Secrets scanning & redaction, retrieval guardrails |
| **LLM0X: Unsafe Outputs** | unsafe_content.txt | Safety classifiers/filters, refusal & safeâ€‘complete |

(Adjust per 2025 OWASP LLM Top 10 list; link in References.)

---

## References & learnâ€‘byâ€‘doing videos

### Primary frameworks & guidance
- **NIST AI RMF 1.0** â€“ official PDF + overview.  
- **OWASP Top 10 for LLMs / GenAI Security Project (2025)** â€“ risks & mitigations.  
- **MITRE ATLAS** â€“ adversarial tactics for AI.  
- **EU AI Act timeline** â€“ implementation dates and GPAI guidance context.  

### Openâ€‘source tools you can plug in
- **PyRIT** â€“ AI redâ€‘team automation framework.  
- **garak** â€“ LLM vulnerability scanner.  
- **NVIDIA NeMo Guardrails** â€“ programmable guardrails for LLM apps.  
- **Llama Prompt Guard 2** â€“ lightweight moderation/safety classifier.  

### YouTube: followâ€‘along builds
- **AI Red Teaming 101** (full playlist + flagship episode) â€“ prompt injection, multiâ€‘turn attacks, automation with PyRIT.  
- **PyRIT deep dives** â€“ handsâ€‘on talks/howâ€‘tos.  
- **garak tutorials** â€“ run scans against your own API.  
- **NeMo Guardrails walkthroughs** â€“ endâ€‘toâ€‘end guardrails.  
- **NIST AI RMF explainers** â€“ concise intros.  
- **OWASP LLM Top 10 (2025) videos** â€“ updates & short demos.  

---

## Security & CI
	â€¢	Unit tests for filters/policies (/evals/tests).  
	â€¢	CI gates (GitHub Actions) to fail on eval regression.  
	â€¢	Optional: CodeQL and OpenSSF Scorecard workflows.  

---

## Contributing

PRs welcome! Please add new tests under `/evals/prompts`, update `/evals/configs/*.yaml`, and map them to OWASP/ATLAS labels.  

---

## License

MIT (see LICENSE).
